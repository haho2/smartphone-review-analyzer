from youtubesearchpython import VideosSearch
import requests
from bs4 import BeautifulSoup
import re

def parse_view_count(view_text):
    """
    'ì¡°íšŒìˆ˜ 120ë§ŒíšŒ', '1.2M views' ê°™ì€ ë¬¸ìì—´ì„ ìˆ«ì(1200000)ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜
    (ì •ë ¬ì„ ìœ„í•´ í•„ìš”)
    """
    if not view_text: return 0
    try:
        return 0 
    except:
        return 0

def search_youtube_top3(keyword):
    """
    í‚¤ì›Œë“œë¡œ ê²€ìƒ‰ í›„ 'ì¡°íšŒìˆ˜'ê°€ ë†’ì€ ì˜ìƒ 3ê°œì˜ IDì™€ ì œëª©ì„ ë°˜í™˜
    ì œí’ˆëª… ì •ê·œí™” ë° ë³€í˜• ê²€ìƒ‰ ì ìš©
    """
    try:
        import product_normalizer
        
        # ì œí’ˆëª… ì •ê·œí™”
        normalized_keyword = product_normalizer.normalize_product_name(keyword)
        
        # ê²€ìƒ‰ ë³€í˜• ìƒì„± (í•œêµ­ì–´ + ì˜ì–´)
        search_variations = product_normalizer.get_product_variations(normalized_keyword)
        
        # ìœ íŠœë¸Œ ê²€ìƒ‰ì€ ì •ê·œí™”ëœ ì œí’ˆëª… + ì˜ì–´ ë³€í˜• ì‚¬ìš©
        search_queries = [
            f"{normalized_keyword} review",
            f"{normalized_keyword} ë¦¬ë·°",
        ]
        
        # ì˜ì–´ ë³€í˜• ì¶”ê°€
        if 'ê°¤ëŸ­ì‹œ' in normalized_keyword:
            # "ê°¤ëŸ­ì‹œ S25" -> "Galaxy S25 review"
            match = __import__('re').search(r'ê°¤ëŸ­ì‹œ\s*(.+)', normalized_keyword)
            if match:
                suffix = match.group(1)
                search_queries.append(f"Galaxy {suffix} review")
        elif 'ì•„ì´í°' in normalized_keyword:
            # "ì•„ì´í° 17" -> "iPhone 17 review"
            match = __import__('re').search(r'ì•„ì´í°\s*(.+)', normalized_keyword)
            if match:
                suffix = match.group(1)
                search_queries.append(f"iPhone {suffix} review")
        
        all_videos = []
        seen_video_ids = set()
        
        # ì—¬ëŸ¬ ê²€ìƒ‰ì–´ë¡œ ê²€ìƒ‰í•˜ì—¬ ë” ë§ì€ ê²°ê³¼ ìˆ˜ì§‘
        for search_query in search_queries[:3]:  # ìƒìœ„ 3ê°œ ê²€ìƒ‰ì–´ ì‚¬ìš©
            try:
                videosSearch = VideosSearch(search_query, limit=10)
                results = videosSearch.result()['result']
                
                for video in results:
                    v_id = video['id']
                    if v_id not in seen_video_ids:
                        all_videos.append(video)
                        seen_video_ids.add(v_id)
            except:
                continue
        
        if not all_videos:
            # ê¸°ë³¸ ê²€ìƒ‰ì–´ë¡œ í•œ ë²ˆ ë” ì‹œë„
            search_query = f"{normalized_keyword} review"
            videosSearch = VideosSearch(search_query, limit=10)
            results = videosSearch.result()['result']
            all_videos = results
        
        video_list = []
        
        for video in all_videos:
            # 2. í•„ìš”í•œ ì •ë³´ë§Œ ì¶”ì¶œ
            v_id = video['id']
            title = video['title']
            view_text = video.get('viewCount', {}).get('text', '0')
            
            # 3. 'Shorts'ëŠ” ì œì™¸í•˜ëŠ” í•„í„°ë§ (ë¦¬ë·° ë¶„ì„ì— ë°©í•´ë¨)
            if 'shorts' not in title.lower():
                video_list.append({
                    'id': v_id,
                    'title': title,
                    'views': view_text # ì •ë ¬ì€ ë³µì¡í•˜ë‹ˆ ì¼ë‹¨ ê²€ìƒ‰ ìƒìœ„ê¶Œ ì‚¬ìš©
                })

        # 4. ìƒìœ„ 3ê°œë§Œ ìë¥´ê¸°
        return video_list[:3]
        
    except Exception as e:
        print(f"âŒ ìœ íŠœë¸Œ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        return []

def crawl_clien(keyword):
    """
    í´ë¦¬ì•™(Clien) ì»¤ë®¤ë‹ˆí‹°ì—ì„œ ì œí’ˆ í›„ê¸° í¬ë¡¤ë§
    """
    try:
        from urllib.parse import quote
        search_query = f"{keyword} í›„ê¸°"
        encoded_query = quote(search_query)
        
        # í´ë¦¬ì•™ ê²€ìƒ‰ URL
        url = f"https://www.clien.net/service/search?q={encoded_query}&sort=recency&boardCd=&isBoard=false"
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.9'
        }
        
        res = requests.get(url, headers=headers, timeout=10)
        res.raise_for_status()
        soup = BeautifulSoup(res.text, 'html.parser')
        
        reviews = []
        # í´ë¦¬ì•™ ê²€ìƒ‰ ê²°ê³¼ íŒŒì‹± (ì—¬ëŸ¬ ì„ íƒì ì‹œë„)
        selectors = [
            '.list_item',
            '.list_row', 
            '.subject_fixed',
            '.list_subject',
            'a[href*="/service/board"]',
            '.title_subject'
        ]
        
        all_items = []
        for selector in selectors:
            items = soup.select(selector)
            for item in items:
                text = item.get_text().strip()
                if len(text) > 15:
                    all_items.append(text)
        
        # ì¤‘ë³µ ì œê±° ë° í•„í„°ë§
        seen = set()
        for text in all_items:
            if len(text) > 20 and keyword.lower() in text.lower():
                if text not in seen:
                    reviews.append(f"[í´ë¦¬ì•™] {text}")
                    seen.add(text)
        
        return reviews[:15]  # ìµœëŒ€ 15ê°œ
        
    except Exception as e:
        print(f"   âš ï¸ í´ë¦¬ì•™ í¬ë¡¤ë§ ì‹¤íŒ¨: {str(e)}")
        return []


def crawl_naver_blog(keyword):
    """
    ë„¤ì´ë²„ ë¸”ë¡œê·¸ì—ì„œ ì œí’ˆ í›„ê¸° í¬ë¡¤ë§ (ê³µê°œ ë¸”ë¡œê·¸ë§Œ)
    """
    try:
        from urllib.parse import quote
        search_query = f"{keyword} ì‹¤ì‚¬ìš© í›„ê¸°"
        encoded_query = quote(search_query)
        
        # ë„¤ì´ë²„ ë¸”ë¡œê·¸ ê²€ìƒ‰ URL
        url = f"https://search.naver.com/search.naver?where=post&query={encoded_query}&sm=tab_jum"
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.9'
        }
        
        res = requests.get(url, headers=headers, timeout=10)
        res.raise_for_status()
        soup = BeautifulSoup(res.text, 'html.parser')
        
        reviews = []
        # ë„¤ì´ë²„ ë¸”ë¡œê·¸ ê²€ìƒ‰ ê²°ê³¼ íŒŒì‹±
        selectors = [
            '.api_txt_lines',
            '.total_tit',
            '.sh_blog_title',
            '.title_link',
            '.title_desc',
            'a.title_link',
            '.sh_blog_passage'
        ]
        
        all_items = []
        for selector in selectors:
            items = soup.select(selector)
            for item in items:
                text = item.get_text().strip()
                if len(text) > 15:
                    all_items.append(text)
        
        # ì¤‘ë³µ ì œê±° ë° í•„í„°ë§
        seen = set()
        for text in all_items:
            if len(text) > 20 and keyword.lower() in text.lower():
                if text not in seen:
                    reviews.append(f"[ë„¤ì´ë²„ ë¸”ë¡œê·¸] {text}")
                    seen.add(text)
        
        return reviews[:20]  # ìµœëŒ€ 20ê°œ
        
    except Exception as e:
        print(f"   âš ï¸ ë„¤ì´ë²„ ë¸”ë¡œê·¸ í¬ë¡¤ë§ ì‹¤íŒ¨: {str(e)}")
        return []


def crawl_dcinside_galaxy(keyword):
    """
    ë””ì‹œì¸ì‚¬ì´ë“œ ê°¤ëŸ­ì‹œ ê°¤ëŸ¬ë¦¬ í›„ê¸° íƒ­ì—ì„œ ì œí’ˆ í›„ê¸° í¬ë¡¤ë§
    """
    try:
        from urllib.parse import quote
        # ê°¤ëŸ­ì‹œ ê°¤ëŸ¬ë¦¬ í›„ê¸° íƒ­ URL
        url = "https://gall.dcinside.com/board/lists/?id=galaxy&page=1&exception_mode=recommend"
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.9',
            'Referer': 'https://gall.dcinside.com/'
        }
        
        res = requests.get(url, headers=headers, timeout=10)
        res.raise_for_status()
        soup = BeautifulSoup(res.text, 'html.parser')
        
        reviews = []
        # ë””ì‹œì¸ì‚¬ì´ë“œ ê°¤ëŸ­ì‹œ ê°¤ëŸ¬ë¦¬ í›„ê¸° íƒ­ íŒŒì‹±
        selectors = [
            '.gall_list .gall_tit a',
            '.ub-content .gall_tit',
            'td.gall_tit a',
            '.list_subject a'
        ]
        
        all_items = []
        for selector in selectors:
            items = soup.select(selector)
            for item in items:
                text = item.get_text().strip()
                if len(text) > 15 and keyword.lower() in text.lower():
                    all_items.append(text)
        
        # ì¤‘ë³µ ì œê±°
        seen = set()
        for text in all_items:
            if text not in seen:
                reviews.append(f"[ë””ì‹œ ê°¤ëŸ­ì‹œ ê°¤ëŸ¬ë¦¬] {text}")
                seen.add(text)
        
        return reviews[:15]  # ìµœëŒ€ 15ê°œ
        
    except Exception as e:
        print(f"   âš ï¸ ë””ì‹œ ê°¤ëŸ­ì‹œ ê°¤ëŸ¬ë¦¬ í¬ë¡¤ë§ ì‹¤íŒ¨: {str(e)}")
        return []


def crawl_dcinside_iphone(keyword):
    """
    ë””ì‹œì¸ì‚¬ì´ë“œ ì•„ì´í° ê°¤ëŸ¬ë¦¬ì—ì„œ ê²€ìƒ‰ìœ¼ë¡œ ì œí’ˆ í›„ê¸° í¬ë¡¤ë§
    """
    try:
        from urllib.parse import quote, urlencode
        
        # ê²€ìƒ‰ì–´ì—ì„œ "í›„ê¸°" ì œê±° (ë””ì‹œì¸ì‚¬ì´ë“œ ê²€ìƒ‰ì´ ë” ì˜ ë¨)
        search_keyword = keyword.replace(" í›„ê¸°", "").replace("í›„ê¸°", "").strip()
        if not search_keyword:
            search_keyword = keyword
        
        # ë””ì‹œì¸ì‚¬ì´ë“œ ê²€ìƒ‰ URL (ì˜¬ë°”ë¥¸ í˜•ì‹)
        # URL íŒŒë¼ë¯¸í„°ë¥¼ ì˜¬ë°”ë¥´ê²Œ êµ¬ì„±
        params = {
            'q': search_keyword,
            's_type': 'all',
            'q_type': 'all',
            'c_id': 'iphone'
        }
        url = f"https://search.dcinside.com/post/q/{quote(search_keyword)}?{urlencode(params)}"
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.9',
            'Referer': 'https://gall.dcinside.com/'
        }
        
        res = requests.get(url, headers=headers, timeout=10)
        res.raise_for_status()
        soup = BeautifulSoup(res.text, 'html.parser')
        
        reviews = []
        # ë””ì‹œì¸ì‚¬ì´ë“œ ì•„ì´í° ê°¤ëŸ¬ë¦¬ ê²€ìƒ‰ ê²°ê³¼ íŒŒì‹±
        selectors = [
            '.sch_result_list .sch_txt',
            '.sch_result_list .sch_tit',
            '.list_subject',
            'a.subject_fixed',
            '.search_result .title',
            '.search_result a'
        ]
        
        all_items = []
        for selector in selectors:
            items = soup.select(selector)
            for item in items:
                text = item.get_text().strip()
                if len(text) > 15:
                    all_items.append(text)
        
        # ì¤‘ë³µ ì œê±° ë° í•„í„°ë§
        seen = set()
        for text in all_items:
            # í‚¤ì›Œë“œê°€ í¬í•¨ë˜ì–´ ìˆê±°ë‚˜, "í›„ê¸°" ê´€ë ¨ í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ í¬í•¨
            if len(text) > 20:
                keyword_lower = keyword.lower()
                text_lower = text.lower()
                if (keyword_lower in text_lower or 
                    'í›„ê¸°' in text_lower or 
                    'ë¦¬ë·°' in text_lower or
                    'ì‚¬ìš©' in text_lower):
                    if text not in seen:
                        reviews.append(f"[ë””ì‹œ ì•„ì´í° ê°¤ëŸ¬ë¦¬] {text}")
                        seen.add(text)
        
        return reviews[:15]  # ìµœëŒ€ 15ê°œ
        
    except Exception as e:
        print(f"   âš ï¸ ë””ì‹œ ì•„ì´í° ê°¤ëŸ¬ë¦¬ í¬ë¡¤ë§ ì‹¤íŒ¨: {str(e)}")
        return []


def crawl_samsung_members(keyword):
    """
    ì‚¼ì„± ë©¤ë²„ìŠ¤ ì»¤ë®¤ë‹ˆí‹°ì—ì„œ ì œí’ˆ í›„ê¸° í¬ë¡¤ë§
    ì‚¼ì„± ë©¤ë²„ìŠ¤ëŠ” ë¡œê·¸ì¸ì´ í•„ìš”í•˜ê±°ë‚˜ URLì´ ë³€ê²½ë˜ì—ˆì„ ìˆ˜ ìˆì–´ì„œ ë„¤ì´ë²„ ê²€ìƒ‰ìœ¼ë¡œ ëŒ€ì²´
    """
    try:
        from urllib.parse import quote
        search_query = f"{keyword} í›„ê¸° site:r1.community.samsung.com"
        encoded_query = quote(search_query)
        
        # ë„¤ì´ë²„ ê²€ìƒ‰ì„ í†µí•´ ì‚¼ì„± ë©¤ë²„ìŠ¤ ê²Œì‹œê¸€ ê²€ìƒ‰
        url = f"https://search.naver.com/search.naver?where=web&query={encoded_query}"
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.9'
        }
        
        res = requests.get(url, headers=headers, timeout=10)
        res.raise_for_status()
        soup = BeautifulSoup(res.text, 'html.parser')
        
        reviews = []
        # ë„¤ì´ë²„ ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ì‚¼ì„± ë©¤ë²„ìŠ¤ ë§í¬ íŒŒì‹±
        selectors = [
            '.api_txt_lines',
            '.total_tit',
            '.sh_web_title',
            'a[href*="community.samsung.com"]'
        ]
        
        all_items = []
        for selector in selectors:
            items = soup.select(selector)
            for item in items:
                text = item.get_text().strip()
                if len(text) > 15:
                    all_items.append(text)
        
        # ì¤‘ë³µ ì œê±° ë° í•„í„°ë§
        seen = set()
        for text in all_items:
            if len(text) > 20 and keyword.lower() in text.lower():
                if text not in seen:
                    reviews.append(f"[ì‚¼ì„± ë©¤ë²„ìŠ¤] {text}")
                    seen.add(text)
        
        return reviews[:15]  # ìµœëŒ€ 15ê°œ
        
    except Exception as e:
        print(f"   âš ï¸ ì‚¼ì„± ë©¤ë²„ìŠ¤ í¬ë¡¤ë§ ì‹¤íŒ¨: {str(e)}")
        return []


def crawl_naver_cafe_iphone(keyword):
    """
    ë„¤ì´ë²„ ì¹´í˜ - ì•„ì´í° ì‚¬ìš©ì ëª¨ì„ì—ì„œ ì œí’ˆ í›„ê¸° í¬ë¡¤ë§
    ì¹´í˜ URL: https://cafe.naver.com/appleiphone
    """
    try:
        from urllib.parse import quote
        search_query = f"{keyword} í›„ê¸°"
        encoded_query = quote(search_query)
        
        # ë„¤ì´ë²„ ì¹´í˜ ê²€ìƒ‰ URL (ì•„ì´í° ì‚¬ìš©ì ëª¨ì„)
        # ê³µê°œ ê²Œì‹œê¸€ë§Œ ê²€ìƒ‰
        url = f"https://search.naver.com/search.naver?where=article&query={encoded_query}+site:cafe.naver.com/appleiphone"
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.9'
        }
        
        res = requests.get(url, headers=headers, timeout=10)
        res.raise_for_status()
        soup = BeautifulSoup(res.text, 'html.parser')
        
        reviews = []
        # ë„¤ì´ë²„ ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ì¹´í˜ ê²Œì‹œê¸€ íŒŒì‹±
        selectors = [
            '.api_txt_lines',
            '.total_tit',
            '.sh_cafe_title',
            '.title_link',
            '.title_desc',
            'a[href*="cafe.naver.com/appleiphone"]'
        ]
        
        all_items = []
        for selector in selectors:
            items = soup.select(selector)
            for item in items:
                text = item.get_text().strip()
                if len(text) > 15:
                    all_items.append(text)
        
        # ì¤‘ë³µ ì œê±° ë° í•„í„°ë§
        seen = set()
        for text in all_items:
            if len(text) > 20 and keyword.lower() in text.lower():
                if text not in seen:
                    reviews.append(f"[ì•„ì´í° ì‚¬ìš©ì ëª¨ì„] {text}")
                    seen.add(text)
        
        return reviews[:15]  # ìµœëŒ€ 15ê°œ
        
    except Exception as e:
        print(f"   âš ï¸ ë„¤ì´ë²„ ì¹´í˜(ì•„ì´í°) í¬ë¡¤ë§ ì‹¤íŒ¨: {str(e)}")
        return []


def crawl_ppomppu(keyword):
    """
    ë½ë¿Œ(Ppomppu) ì»¤ë®¤ë‹ˆí‹°ì—ì„œ ì œí’ˆ í›„ê¸° í¬ë¡¤ë§
    """
    try:
        from urllib.parse import quote
        search_query = f"{keyword} í›„ê¸°"
        encoded_query = quote(search_query)
        
        # ë½ë¿Œ ê²€ìƒ‰ URL
        url = f"https://www.ppomppu.co.kr/search_bbs.php?search_type=sub_memo&keyword={encoded_query}"
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.9'
        }
        
        res = requests.get(url, headers=headers, timeout=10)
        res.raise_for_status()
        soup = BeautifulSoup(res.text, 'html.parser')
        
        reviews = []
        # ë½ë¿Œ ê²€ìƒ‰ ê²°ê³¼ íŒŒì‹± (ì—¬ëŸ¬ ì„ íƒì ì‹œë„)
        selectors = [
            '.title',
            '.subject',
            '.list_title',
            'a[href*="/zboard/view"]',
            '.board_list .title',
            'td.title'
        ]
        
        all_items = []
        for selector in selectors:
            items = soup.select(selector)
            for item in items:
                text = item.get_text().strip()
                if len(text) > 15:
                    all_items.append(text)
        
        # ì¤‘ë³µ ì œê±° ë° í•„í„°ë§
        seen = set()
        for text in all_items:
            if len(text) > 20 and keyword.lower() in text.lower():
                if text not in seen:
                    reviews.append(f"[ë½ë¿Œ] {text}")
                    seen.add(text)
        
        return reviews[:15]  # ìµœëŒ€ 15ê°œ
        
    except Exception as e:
        print(f"   âš ï¸ ë½ë¿Œ í¬ë¡¤ë§ ì‹¤íŒ¨: {str(e)}")
        return []


def crawl_community_reviews(keyword):
    """
    ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì»¤ë®¤ë‹ˆí‹° ì‚¬ì´íŠ¸ì—ì„œ ì§ì ‘ ì œí’ˆ í›„ê¸° í¬ë¡¤ë§ (ë¹…ë°ì´í„° ìˆ˜ì§‘)
    
    ë°ì´í„° ì†ŒìŠ¤:
    - í´ë¦¬ì•™ (clien.net): IT/ì „ìì œí’ˆ ì „ë¬¸ ì»¤ë®¤ë‹ˆí‹°
    - ë½ë¿Œ (ppomppu.co.kr): ì‡¼í•‘/ì œí’ˆ í›„ê¸° ì»¤ë®¤ë‹ˆí‹°
    - ë„¤ì´ë²„ ë¸”ë¡œê·¸: ê³µê°œ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸
    - ì‚¼ì„± ë©¤ë²„ìŠ¤: ì‚¼ì„± ì œí’ˆ ì‚¬ìš©ì ì»¤ë®¤ë‹ˆí‹°
    - ë„¤ì´ë²„ ì¹´í˜ - ì•„ì´í° ì‚¬ìš©ì ëª¨ì„: ì•„ì´í° ì‚¬ìš©ì ì „ìš© ì»¤ë®¤ë‹ˆí‹°
    - ë””ì‹œì¸ì‚¬ì´ë“œ: ê°¤ëŸ­ì‹œëŠ” ê°¤ëŸ­ì‹œ ê°¤ëŸ¬ë¦¬ í›„ê¸° íƒ­, ì•„ì´í°ì€ ì•„ì´í° ê°¤ëŸ¬ë¦¬ ê²€ìƒ‰
    
    ìµœëŒ€ 50ê°œ ì´ìƒì˜ í›„ê¸°ë¥¼ ìˆ˜ì§‘í•˜ì—¬ ë¹…ë°ì´í„° ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    
    ì œí’ˆëª… ë³€í˜•ì„ ìë™ìœ¼ë¡œ ì²˜ë¦¬í•˜ì—¬ ë‹¤ì–‘í•œ ê²€ìƒ‰ì–´ë¡œ í¬ë¡¤ë§í•©ë‹ˆë‹¤.
    """
    import product_normalizer
    
    # ì œí’ˆëª… ì •ê·œí™” ë° ë³€í˜• ìƒì„±
    normalized_keyword = product_normalizer.normalize_product_name(keyword)
    search_variations = product_normalizer.get_product_variations(normalized_keyword)
    
    print(f"   ğŸ“ ê²€ìƒ‰ ë³€í˜•: {', '.join(search_variations[:3])}...")
    
    all_reviews = []
    sources = []
    
    try:
        # 1. í´ë¦¬ì•™ í¬ë¡¤ë§ (ì—¬ëŸ¬ ë³€í˜•ìœ¼ë¡œ ê²€ìƒ‰)
        print(f"   â†’ í´ë¦¬ì•™ í¬ë¡¤ë§ ì¤‘...")
        clien_reviews = []
        seen_clien = set()
        for variation in search_variations[:3]:  # ìƒìœ„ 3ê°œ ë³€í˜•ë§Œ ì‚¬ìš©
            reviews = crawl_clien(variation)
            for review in reviews:
                if review not in seen_clien:
                    clien_reviews.append(review)
                    seen_clien.add(review)
        if clien_reviews:
            all_reviews.extend(clien_reviews)
            sources.append(f"í´ë¦¬ì•™ ({len(clien_reviews)}ê°œ)")
            print(f"      âœ… í´ë¦¬ì•™ì—ì„œ {len(clien_reviews)}ê°œ í›„ê¸° ë°œê²¬")
        
        # 2. ë½ë¿Œ í¬ë¡¤ë§ (ì—¬ëŸ¬ ë³€í˜•ìœ¼ë¡œ ê²€ìƒ‰)
        print(f"   â†’ ë½ë¿Œ í¬ë¡¤ë§ ì¤‘...")
        ppomppu_reviews = []
        seen_ppomppu = set()
        for variation in search_variations[:3]:
            reviews = crawl_ppomppu(variation)
            for review in reviews:
                if review not in seen_ppomppu:
                    ppomppu_reviews.append(review)
                    seen_ppomppu.add(review)
        if ppomppu_reviews:
            all_reviews.extend(ppomppu_reviews)
            sources.append(f"ë½ë¿Œ ({len(ppomppu_reviews)}ê°œ)")
            print(f"      âœ… ë½ë¿Œì—ì„œ {len(ppomppu_reviews)}ê°œ í›„ê¸° ë°œê²¬")
        
        # 3. ë„¤ì´ë²„ ë¸”ë¡œê·¸ í¬ë¡¤ë§ (ì—¬ëŸ¬ ë³€í˜•ìœ¼ë¡œ ê²€ìƒ‰)
        print(f"   â†’ ë„¤ì´ë²„ ë¸”ë¡œê·¸ í¬ë¡¤ë§ ì¤‘...")
        naver_blog_reviews = []
        seen_naver_blog = set()
        for variation in search_variations[:3]:
            reviews = crawl_naver_blog(variation)
            for review in reviews:
                if review not in seen_naver_blog:
                    naver_blog_reviews.append(review)
                    seen_naver_blog.add(review)
        if naver_blog_reviews:
            all_reviews.extend(naver_blog_reviews)
            sources.append(f"ë„¤ì´ë²„ ë¸”ë¡œê·¸ ({len(naver_blog_reviews)}ê°œ)")
            print(f"      âœ… ë„¤ì´ë²„ ë¸”ë¡œê·¸ì—ì„œ {len(naver_blog_reviews)}ê°œ í›„ê¸° ë°œê²¬")
        
        # 4. ì‚¼ì„± ë©¤ë²„ìŠ¤ í¬ë¡¤ë§ (ì—¬ëŸ¬ ë³€í˜•ìœ¼ë¡œ ê²€ìƒ‰)
        print(f"   â†’ ì‚¼ì„± ë©¤ë²„ìŠ¤ í¬ë¡¤ë§ ì¤‘...")
        samsung_reviews = []
        seen_samsung = set()
        for variation in search_variations[:3]:
            reviews = crawl_samsung_members(variation)
            for review in reviews:
                if review not in seen_samsung:
                    samsung_reviews.append(review)
                    seen_samsung.add(review)
        if samsung_reviews:
            all_reviews.extend(samsung_reviews)
            sources.append(f"ì‚¼ì„± ë©¤ë²„ìŠ¤ ({len(samsung_reviews)}ê°œ)")
            print(f"      âœ… ì‚¼ì„± ë©¤ë²„ìŠ¤ì—ì„œ {len(samsung_reviews)}ê°œ í›„ê¸° ë°œê²¬")
        
        # 5. ë„¤ì´ë²„ ì¹´í˜ - ì•„ì´í° ì‚¬ìš©ì ëª¨ì„ í¬ë¡¤ë§ (ì—¬ëŸ¬ ë³€í˜•ìœ¼ë¡œ ê²€ìƒ‰)
        print(f"   â†’ ë„¤ì´ë²„ ì¹´í˜(ì•„ì´í°) í¬ë¡¤ë§ ì¤‘...")
        iphone_cafe_reviews = []
        seen_iphone_cafe = set()
        for variation in search_variations[:3]:
            reviews = crawl_naver_cafe_iphone(variation)
            for review in reviews:
                if review not in seen_iphone_cafe:
                    iphone_cafe_reviews.append(review)
                    seen_iphone_cafe.add(review)
        if iphone_cafe_reviews:
            all_reviews.extend(iphone_cafe_reviews)
            sources.append(f"ì•„ì´í° ì‚¬ìš©ì ëª¨ì„ ({len(iphone_cafe_reviews)}ê°œ)")
            print(f"      âœ… ì•„ì´í° ì‚¬ìš©ì ëª¨ì„ì—ì„œ {len(iphone_cafe_reviews)}ê°œ í›„ê¸° ë°œê²¬")
        
        # 6. ë””ì‹œì¸ì‚¬ì´ë“œ í¬ë¡¤ë§ (ì œí’ˆì— ë”°ë¼ ë‹¤ë¥´ê²Œ ì²˜ë¦¬, ì—¬ëŸ¬ ë³€í˜•ìœ¼ë¡œ ê²€ìƒ‰)
        keyword_lower = normalized_keyword.lower()
        if 'ê°¤ëŸ­ì‹œ' in keyword_lower or 'galaxy' in keyword_lower or 'samsung' in keyword_lower:
            print(f"   â†’ ë””ì‹œ ê°¤ëŸ­ì‹œ ê°¤ëŸ¬ë¦¬ í¬ë¡¤ë§ ì¤‘...")
            dc_galaxy_reviews = []
            seen_dc_galaxy = set()
            for variation in search_variations[:2]:  # ë””ì‹œëŠ” ë³€í˜•ì´ ì ê²Œ í•„ìš”
                reviews = crawl_dcinside_galaxy(variation)
                for review in reviews:
                    if review not in seen_dc_galaxy:
                        dc_galaxy_reviews.append(review)
                        seen_dc_galaxy.add(review)
            if dc_galaxy_reviews:
                all_reviews.extend(dc_galaxy_reviews)
                sources.append(f"ë””ì‹œ ê°¤ëŸ­ì‹œ ê°¤ëŸ¬ë¦¬ ({len(dc_galaxy_reviews)}ê°œ)")
                print(f"      âœ… ë””ì‹œ ê°¤ëŸ­ì‹œ ê°¤ëŸ¬ë¦¬ì—ì„œ {len(dc_galaxy_reviews)}ê°œ í›„ê¸° ë°œê²¬")
        elif 'ì•„ì´í°' in keyword_lower or 'iphone' in keyword_lower or 'ì• í”Œ' in keyword_lower:
            print(f"   â†’ ë””ì‹œ ì•„ì´í° ê°¤ëŸ¬ë¦¬ í¬ë¡¤ë§ ì¤‘...")
            dc_iphone_reviews = []
            seen_dc_iphone = set()
            for variation in search_variations[:3]:
                reviews = crawl_dcinside_iphone(variation)
                for review in reviews:
                    if review not in seen_dc_iphone:
                        dc_iphone_reviews.append(review)
                        seen_dc_iphone.add(review)
            if dc_iphone_reviews:
                all_reviews.extend(dc_iphone_reviews)
                sources.append(f"ë””ì‹œ ì•„ì´í° ê°¤ëŸ¬ë¦¬ ({len(dc_iphone_reviews)}ê°œ)")
                print(f"      âœ… ë””ì‹œ ì•„ì´í° ê°¤ëŸ¬ë¦¬ì—ì„œ {len(dc_iphone_reviews)}ê°œ í›„ê¸° ë°œê²¬")
        
        if all_reviews:
            actual_count = len(all_reviews)
            print(f"   âœ… ì´ {actual_count}ê°œ í›„ê¸° ìˆ˜ì§‘ ì™„ë£Œ")
            result_text = f"[ë°ì´í„° ì†ŒìŠ¤: {', '.join(sources)}]\n\n"
            result_text += "\n".join(all_reviews[:50])  # ìµœëŒ€ 50ê°œë¡œ ì¦ê°€ (ë¹…ë°ì´í„°!)
            return result_text, sources, actual_count  # ì‹¤ì œ ê°œìˆ˜ë„ ë°˜í™˜
        else:
            print(f"   âš ï¸ í›„ê¸°ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            return "ì»¤ë®¤ë‹ˆí‹° ë¦¬ë·°ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.", [], 0
        
    except Exception as e:
        print(f"   âŒ ì»¤ë®¤ë‹ˆí‹° í¬ë¡¤ë§ ì‹¤íŒ¨: {str(e)}")
        import traceback
        traceback.print_exc()
        return "ì»¤ë®¤ë‹ˆí‹° ë¦¬ë·°ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.", [], 0